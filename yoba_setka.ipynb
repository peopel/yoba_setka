{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    }
   ],
   "source": [
    "#######1\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from nlpia.loaders import get_data\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import io\n",
    "from random import shuffle\n",
    "def pre_process_data(filepath):\n",
    "    positive_path = os.path.join(filepath, 'pos')\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    dataset = []\n",
    "    \n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
    "        with io.open(filename, 'r', encoding='utf-8') as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "    \n",
    "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
    "        with io.open(filename, 'r', encoding='utf-8') as f:       \n",
    "            dataset.append((neg_label, f.read()))\n",
    "    \n",
    "    shuffle(dataset)\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize3(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            \n",
    "            except KeyError:\n",
    "                pass # net slova\n",
    "            \n",
    "        vectorized_data.append(sample_vecs)\n",
    "    \n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    #\"\"\" Выбираем целевые значения из набора данных \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nlpia.loaders:Downloading w2v\n",
      "INFO:nlpia.web:URL too short: w2v\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.300d\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.27b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.42b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.6b\\.zip$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.bin\\.gz$, string=googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.tgz$, string=googlenews-vectors-negative300.bin.gz\n",
      "INFO:nlpia.loaders:expanded+normalized file path: C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:nlpia.loaders:requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
      "INFO:nlpia.loaders:remote_size: 1647046227\n",
      "INFO:nlpia.loaders:local_size: 1647046227\n",
      "INFO:nlpia.loaders:retained: C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "WARNING:nlpia.loaders:normalize_ext.filepath=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.300d\\.zip$, string=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.27b\\.zip$, string=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.42b\\.zip$, string=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.6b\\.zip$, string=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.bin\\.gz$, string=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.futil:regex pattern = ^[.]?([^.]*)\\.([^.]{1,10})*\\.tgz$, string=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "WARNING:nlpia.loaders:download_unzip.new_filepaths=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.rename_file(source=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz, dest=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz)\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.os.rename(source=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz, dest=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz)\n",
      "WARNING:nlpia.loaders:download_unzip.filepath=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:downloaded name=w2v to filepath=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.get_data.filepaths={'w2v': 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\nlpia\\\\bigdata\\\\googlenews-vectors-negative300.bin.gz'}\n",
      "DEBUG:nlpia.loaders:nlpia.loaders.get_data.filepath=C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:gensim.models.keyedvectors:loading projection weights from C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nlpia\\bigdata\\googlenews-vectors-negative300.bin.gz\n",
      "INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\nlpia\\\\bigdata\\\\googlenews-vectors-negative300.bin.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2021-09-15T19:37:08.889829', 'gensim': '4.1.0', 'python': '3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "word_vectors = get_data('w2v', limit = 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    new_data = []\n",
    "    \n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "        \n",
    "    for sample in data:\n",
    "        if(len(sample)) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 400, 50)           70200     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 20001     \n",
      "=================================================================\n",
      "Total params: 90,201\n",
      "Trainable params: 90,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "625/625 [==============================] - 93s 143ms/step - loss: 0.4694 - accuracy: 0.7831 - val_loss: 0.3645 - val_accuracy: 0.8508\n",
      "Epoch 2/2\n",
      "625/625 [==============================] - 73s 116ms/step - loss: 0.3403 - accuracy: 0.8572 - val_loss: 0.3311 - val_accuracy: 0.8618\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "dataset = pre_process_data(r'C:\\aclImdb_v1\\aclImdb\\train')\n",
    "vectorized_data = tokenize_and_vectorize3(dataset)\n",
    "expected = collect_expected(dataset)\n",
    "split_point = int(len(vectorized_data) * .8)\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "epochs = 2\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "x_train = np.reshape(x_train,\n",
    "                    (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)\n",
    "num_neurons = 50\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "model_structure = model.to_json()\n",
    "with open(\"lstm_model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "    model.save_weights(\"lstm_weights1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "with open(\"lstm_model1.json\", \"r\") as json_file:\n",
    "    json_string = json_file.read()\n",
    "    model = model_from_json(json_string)\n",
    "    model.load_weights('lstm_weights1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = \"\"\"I hate niggers.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_list = tokenize_and_vectorize3([(1, sample_1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec_list = pad_trunc(vec_list, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_vec)\n",
    "#y_pred = np.round(y_pred).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample's sentiement, 1 - pos, 2 - neg : [[0.41108355]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample's sentiement, 1 - pos, 2 - neg : {}\"\\\n",
    "      .format(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zaebis_li_text(sample):\n",
    "    vec_list = tokenize_and_vectorize3([(1, sample)])\n",
    "    test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "    test_vec = np.reshape(test_vec_list, (len(test_vec_list), \n",
    "                                          maxlen, embedding_dims))\n",
    "    y_pred = model.predict(test_vec)\n",
    "    nascolko_norm = format(y_pred[0][0], '.2f') \n",
    "    print(\"Текст позитивный? 0 - нет, 1 - да : \", nascolko_norm)\n",
    "\n",
    "    return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст позитивный? 0 - нет, 1 - да :  0.10\n",
      "Текст позитивный? 0 - нет, 1 - да :  0.96\n"
     ]
    }
   ],
   "source": [
    "y_pred_2 = zaebis_li_text(\"I hate this disgusting fucking shit\")\n",
    "y_pred_2 = zaebis_li_text(\"I love this beautiful amazing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст позитивный? 0 - нет, 1 - да :  0.36\n"
     ]
    }
   ],
   "source": [
    "y_pred_2 = zaebis_li_text(\"Hitler did nothing wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
